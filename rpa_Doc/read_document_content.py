import json
import os
from urllib.parse import urljoin
from playwright.sync_api import Page, TimeoutError

OUTPUT_DIR = "output"
INPUT_FILE = os.path.join(OUTPUT_DIR, "month_document_urls.json")
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "month_document_contents.json")


# ------------------------------------------------------------------
# Utils: ‡∏≠‡πà‡∏≤‡∏ô field ‡∏à‡∏≤‡∏Å table (‡∏´‡∏ô‡πâ‡∏≤ detail)
# ------------------------------------------------------------------
def extract_field_from_table(page: Page, label: str) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å table row <tr> ‡∏ó‡∏µ‡πà‡∏°‡∏µ label
    ‡πÄ‡∏ä‡πà‡∏ô '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠', '‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠'
    """
    try:
        row = page.locator(
            f"xpath=//tr[td/strong[contains(normalize-space(), '{label}')]]"
        ).first

        if row.count() == 0:
            return ""

        content = row.locator("td").nth(1).inner_text().strip()
        if content.startswith(":"):
            content = content[1:].strip()

        return content

    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '{label}' ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
        return ""


# ------------------------------------------------------------------
# ‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤ list ‡πÅ‡∏ö‡∏ö table (1 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á = 2 tr)
# ------------------------------------------------------------------
def read_documents_from_table_list(page: Page) -> list[dict]:
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤ list ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô table:
    - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ tr ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á'
    - tr ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ = ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ + ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
    - ‡∏ï‡∏±‡∏î pagination / header / footer ‡∏ó‡∏¥‡πâ‡∏á
    """
    results = []

    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î scope ‡πÅ‡∏Ñ‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô header/footer
    container = page.locator("div[id^='c'] table tbody")

    topic_rows = container.locator(
        "xpath=.//tr[td//span[contains(normalize-space(), '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á')]]"
    )

    count = topic_rows.count()
    print(f"üìå ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤ list ‡∏°‡∏µ {count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    for i in range(count):
        row = topic_rows.nth(i)

        # ---- ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á + link ----
        link = row.locator("a").first
        title = link.inner_text().strip()
        href = link.get_attribute("href")
        url = urljoin(page.url, href)

        # ---- tr ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ: ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠ + ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ----
        detail_row = row.locator("xpath=following-sibling::tr[1]")
        detail_text = detail_row.inner_text()

        def extract(label: str) -> str:
            if label in detail_text:
                return (
                    detail_text
                    .split(label, 1)[1]
                    .strip()
                    .split("\n")[0]
                )
            return ""

        book_no = extract("‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠")
        date = extract("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà")

        results.append({
            "title": title,
            "url": url,
            "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠": book_no,
            "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": date,
        })

    return results


# ------------------------------------------------------------------
# ‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ 1 URL (auto detect ‡∏´‡∏ô‡πâ‡∏≤ list / detail)
# ------------------------------------------------------------------
def read_single_document(page: Page, url: str, fallback_title: str):
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=30000)
    except TimeoutError:
        print(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö timeout: {url}")
        return {
            "title": fallback_title,
            "url": url,
            "error": "timeout"
        }

    # -------------------------
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤ list ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # -------------------------
    if page.locator("span:has-text('‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á')").count() > 1:
        print("üìÑ ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤ list ‡πÅ‡∏ö‡∏ö table")
        return read_documents_from_table_list(page)

    # -------------------------
    # ‡∏´‡∏ô‡πâ‡∏≤ detail ‡∏õ‡∏Å‡∏ï‡∏¥
    # -------------------------
    return {
        "title": fallback_title,
        "url": url,
        "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠": extract_field_from_table(page, "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠"),
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": extract_field_from_table(page, "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"),
        "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á": extract_field_from_table(page, "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"),
        "‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢": extract_field_from_table(page, "‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"),
        "‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠": extract_field_from_table(page, "‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠"),
        "‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢": extract_field_from_table(page, "‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢"),
    }


# ------------------------------------------------------------------
# Main task: ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
# ------------------------------------------------------------------
def run_read_document_content(page: Page):
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        months = json.load(f)

    results = []

    # =========================
    # NEW: ‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    # =========================
    total_links = 0
    total_documents = 0

    for m in months:
        print(f"\nüìÖ ‡∏õ‡∏µ {m['year']} ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô {m['month']}")

        documents = []
        month_documents_count = 0  # NEW: ‡∏ô‡∏±‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô

        for item in m.get("links", []):
            total_links += 1
            print(f"   üîó {item['url']}")

            data = read_single_document(
                page,
                item["url"],
                item.get("title", "")
            )

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‚Üí ‡πÅ‡∏ï‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
            if isinstance(data, list):
                documents.extend(data)
                month_documents_count += len(data)
                total_documents += len(data)
            else:
                documents.append(data)
                month_documents_count += 1
                total_documents += 1

        print(f"   ‚úÖ ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ {month_documents_count} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")

        results.append({
            "year": m["year"],
            "month": m["month"],
            "month_no": m.get("month_no", ""),
            "documents": documents
        })

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # =========================
    # NEW: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    # =========================
    print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    print(f"üîó URL ‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î : {total_links}")
    print(f"üìÑ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á    : {total_documents}")
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà        : {OUTPUT_FILE}")


# ------------------------------------------------------------------
# Debug run (standalone)
# ------------------------------------------------------------------
if __name__ == "__main__":
    from playwright.sync_api import sync_playwright

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        run_read_document_content(page)
        browser.close()
