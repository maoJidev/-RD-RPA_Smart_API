import json
import os
import random
import time
from urllib.parse import urljoin
from playwright.sync_api import Page, TimeoutError as PlaywrightTimeoutError

# =========================
# CONFIG
# =========================
OUTPUT_DIR = "output"
MONTHS_FILE = os.path.join(OUTPUT_DIR, "months.json")
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "rd_discussion_with_count.json")

MAX_DOCS_PER_MONTH = 20     # üëà ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 10 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)
FAST_MODE = True

SLEEP_SHORT = (400, 800)
SLEEP_DETAIL = (800, 1500)
ERROR_SLEEP_SEC = 3


# =========================
# UTILS
# =========================
def human_sleep(page: Page, ms_range):
    page.wait_for_timeout(random.randint(*ms_range))


# =========================
# PARSE DETAIL
# =========================
def parse_document_detail(page: Page) -> dict:
    page.wait_for_selector("table", timeout=10000)

    data = {
        "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠": "",
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": "",
        "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á": "",
        "‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢": "",
        "‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠": "",
        "‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢": ""
    }

    rows = page.locator("table tr").all()  # ‚úÖ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ tbody

    for row in rows:
        tds = row.locator("td").all()
        if len(tds) < 2:
            continue

        key = tds[0].inner_text().strip()
        val = tds[1].inner_text().strip()

        if "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠" in key:
            data["‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠"] = val
        elif "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà" in key:
            data["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"] = val
        elif key.startswith("‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"):
            data["‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"] = val
        elif "‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢" in key:
            data["‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"] = val
        elif "‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠" in key:
            data["‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠"] = val
        elif "‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢" in key:
            data["‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢"] = val

    return data



# =========================
# COLLECT LINKS (ALL PAGES)
# =========================
def collect_all_document_links(page: Page, month_url: str):
    links = []
    visited_pages = set()
    collected_urls = set()

    page.goto(month_url, timeout=20000)
    page.wait_for_load_state("domcontentloaded")

    while True:
        if page.url in visited_pages:
            break
        visited_pages.add(page.url)

        page.wait_for_selector("a", timeout=10000)

        # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö <a> ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ä‡∏µ‡πâ‡πÑ‡∏õ .html
        anchors = page.locator("a[href$='.html']").all()

        for a in anchors:
            try:
                title = a.inner_text().strip()
                href = a.get_attribute("href")

                if not title or not href:
                    continue

                # ‚ùå ‡∏ï‡∏±‡∏î pagination / ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤
                if title.isdigit():
                    continue

                full_url = urljoin(page.url, href)

                # ‚ùå ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥
                if full_url in collected_urls:
                    continue

                collected_urls.add(full_url)

                links.append({
                    "title": title,
                    "url": full_url
                })
            except:
                continue

        # üîÅ ‡∏´‡∏≤ pagination page ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡πÅ‡∏ö‡∏ö generic)
        next_page_found = False
        for a in anchors:
            txt = a.inner_text().strip()
            href = a.get_attribute("href")

            if not href or not txt.isdigit():
                continue

            next_url = urljoin(page.url, href)
            if next_url not in visited_pages:
                page.goto(next_url, timeout=20000)
                page.wait_for_load_state("domcontentloaded")
                next_page_found = True
                break

        if not next_page_found:
            break

    return links


# =========================
# MAIN
# =========================
def scrape_documents_with_count(page: Page):
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    months = [
        {
            "year": "2545",
            "month": "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°",
            "month_no": 10,
            "url": "https://www.rd.go.th/24852.html"
        },
        {
            "year": "2545",
            "month": "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°",
            "month_no": 7,
            "url": "https://www.rd.go.th/24843.html"
        },
        {
            "year": "2545",
            "month": "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô",
            "month_no": 6,
            "url": "https://www.rd.go.th/24844.html"
        },
        {
            "year": "2545",
            "month": "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°",
            "month_no": 5,
            "url": "https://www.rd.go.th/24845.html"
        },
        {
            "year": "2545",
            "month": "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô",
            "month_no": 4,
            "url": "https://www.rd.go.th/24846.html"
        },
        {
            "year": "2545",
            "month": "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°",
            "month_no": 3,
            "url": "https://www.rd.go.th/24847.html"
        }
    ]

    all_results = []

    for m in months:
        print(f"\nüìÑ {m['year']} {m['month']}")

        try:
            links = collect_all_document_links(page, m["url"])
        except Exception as e:
            print(f"‚ö†Ô∏è ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
            continue

        print(f"   üîé ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(links)} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")

        documents = []

        for item in links[:MAX_DOCS_PER_MONTH]:
            try:
                print(f"   ‚ñ∂ {item['title']}")
                page.goto(item["url"], timeout=20000)
                human_sleep(page, SLEEP_DETAIL)

                detail = parse_document_detail(page)

                full_text = (
                    detail["‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"]
                    + detail["‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠"]
                    + detail["‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢"]
                )

                documents.append({
                    "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á": detail["‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"] or item["title"],
                    "‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠": detail["‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠"],
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": detail["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"],
                    "‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢": detail["‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"],
                    "‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠": detail["‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏∑‡∏≠"],
                    "‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢": detail["‡πÅ‡∏ô‡∏ß‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢"],
                    "content_length": len(full_text),
                    "content_lines": len(full_text.splitlines()),
                    "url": item["url"]
                })

            except Exception as e:
                print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á: {e}")
                time.sleep(ERROR_SLEEP_SEC)

        all_results.append({
            "year": m["year"],
            "month": m["month"],
            "month_no": m["month_no"],
            "url": m["url"],
            "total_topics": len(documents),
            "documents": documents
        })

        print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(documents)} ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô -> {OUTPUT_FILE}")
